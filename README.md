# åŸºäº CNN ä¸ LSTM çš„ç»“æ„åŒ–å­—ç¬¦è¯†åˆ«ä¸é¢„æµ‹å»ºæ¨¡

## ğŸ“š Overview | é¡¹ç›®æ¦‚è¿°

This project implements and analyzes multiple neural architectures including MLP, CNN, and RNN/LSTM to handle two types of tasks:  
(1) **Handwritten character image classification** using KMNIST dataset.  
(2) **Structured sequence modeling and prediction**, focusing on formal grammars such as `anbn`, `anbncn`, and Reber Grammar.  
The project explores how different architectures encode features, tracks hidden state transitions, and visualizes decision boundaries and predictions.  

æœ¬é¡¹ç›®å®ç°å¹¶åˆ†æäº†å¤šç§ç¥ç»ç½‘ç»œç»“æ„ï¼ŒåŒ…æ‹¬ MLPã€CNN å’Œ RNN/LSTMï¼Œåˆ†åˆ«ç”¨äºä¸¤ä¸ªä»»åŠ¡ï¼š  
ï¼ˆ1ï¼‰ä½¿ç”¨ KMNIST æ•°æ®é›†è¿›è¡Œ**æ—¥æ–‡æ‰‹å†™å›¾åƒåˆ†ç±»**ï¼›  
ï¼ˆ2ï¼‰æ¨¡æ‹Ÿç»“æ„åŒ–è¯­æ³•ï¼ˆå¦‚ anbnã€anbncn å’Œ Reber Grammarï¼‰è¿›è¡Œ**å­—ç¬¦åºåˆ—å»ºæ¨¡ä¸é¢„æµ‹**ã€‚  
é¡¹ç›®é‡ç‚¹æ¢ç´¢ä¸åŒæ¨¡å‹å¯¹ç‰¹å¾çš„ç¼–ç æ–¹å¼ï¼Œè¿½è¸ªéšè—çŠ¶æ€å˜åŒ–ï¼Œå¹¶é€šè¿‡å¯è§†åŒ–æ‰‹æ®µå±•ç¤ºé¢„æµ‹è¾¹ç•Œä¸åŠ¨æ€ã€‚
---

## ğŸ“ é¡¹ç›®ç»“æ„

```bash
â”œâ”€â”€ NetLin / NetFull / NetConv # å›¾åƒåˆ†ç±»ç½‘ç»œæ¨¡å‹ï¼ˆKMNIST æ•°æ®é›†ï¼‰
â”œâ”€â”€ check.py / check_main.py # MLP æ‰‹åŠ¨è®¾æƒè®­ç»ƒä¸æ¿€æ´»å›¾å¯è§†åŒ–
â”œâ”€â”€ anbn.py # anbn åºåˆ—æ•°æ®ç”Ÿæˆå™¨
â”œâ”€â”€ RNN + LSTM + å¯è§†åŒ–è¾“å‡º # åºåˆ—ç»“æ„å­¦ä¹ ä¸å¯è§£é‡Šæ€§åˆ†æ
â”œâ”€â”€ plot/ # ä¿å­˜å¯è§†åŒ–ç»“æœï¼ˆæ¿€æ´»å›¾ã€è¾“å‡ºå›¾ã€è½¨è¿¹å›¾ç­‰ï¼‰
â”œâ”€â”€ hw1.pdf # æœ€ç»ˆæŠ¥å‘ŠåŠå®éªŒæ€»ç»“
```
---
---

## ğŸ”§ Components | æ¨¡å—è¯´æ˜

### Part 1: KMNIST Image Classification | å›¾åƒåˆ†ç±»
- Implemented three models: `NetLin` (linear), `NetFull` (1 hidden layer), and `NetConv` (CNN).
- Accuracy results:  
  - NetLin: 70%  
  - NetFull: 85%  
  - NetConv: **94%**
- Tools: `PyTorch`, `CrossEntropyLoss`, `ConfusionMatrix`, visualization of misclassified samples.

### Part 2: MLP Manual Design & Visualization | MLP ç½‘ç»œå¯è§†åŒ–ä¸æ‰‹åŠ¨æƒé‡è®¾è®¡
- Built an MLP with manually defined weights (step/sigmoid activation).
- Visualized hidden unit activations (`hid_X_Y.jpg`) and output boundary (`out_X.jpg`).
- Achieved **100% accuracy** on 2D binary classification task.

### Part 3: RNN/LSTM for Grammar Learning | é€’å½’ç½‘ç»œè¯­æ³•å»ºæ¨¡
- Custom sequence generator for `anbn` and `anbncn` structures.
- Trained RNN/LSTM to track symbol order and predict next character.
- Hidden state trajectories visualized to show segmented counting and state reset mechanism.
- Applied LSTM on Reber Grammar to demonstrate **superior long-range dependency modeling**.

---

## ğŸ§  é¡¹ç›®åŠŸèƒ½ä¸æˆæœæ¦‚è§ˆ

### 1. å›¾åƒåˆ†ç±»ï¼ˆKMNISTï¼‰

- ä½¿ç”¨ä¸‰ç§ç»“æ„ï¼šçº¿æ€§æ¨¡å‹ï¼ˆNetLinï¼‰ã€MLPï¼ˆNetFullï¼‰ã€CNNï¼ˆNetConvï¼‰
- å‡†ç¡®ç‡åˆ†åˆ«ä¸º **70% / 85% / 94%**
- å®ç° PyTorch è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒä¸æµ‹è¯•æ¡†æ¶

### 2. MLP å¯è§†åŒ–ä¸é€»è¾‘å»ºæ¨¡

- æ„å»ºå…·æœ‰æ‰‹åŠ¨æƒé‡è®¾ç½®åŠŸèƒ½çš„ MLP äºŒåˆ†ç±»å™¨
- å¯è§†åŒ–æ¯ä¸ªç¥ç»å…ƒåœ¨è¾“å…¥ç©ºé—´ä¸­çš„å“åº”åŒºåŸŸï¼ˆéšè—å±‚æ¿€æ´»å›¾ï¼‰
- æœ€ç»ˆå‡†ç¡®ç‡è¾¾ **100%**

### 3. åºåˆ—å»ºæ¨¡ä¸ LSTM å­¦ä¹ èƒ½åŠ›

- ä½¿ç”¨è‡ªå®šä¹‰ anbnã€anbncn åºåˆ—ç”Ÿæˆå™¨å­¦ä¹ å­—ç¬¦è®¡æ•°æœºåˆ¶
- å¯è§†åŒ– RNN çš„éšè—çŠ¶æ€å˜åŒ–ä¸é¢„æµ‹è¾“å‡ºçƒ­åŠ›å›¾
- ä½¿ç”¨ LSTM æˆåŠŸå­¦ä¹  Reber Grammar çš„åµŒå¥—ç»“æ„ï¼Œè¡¨ç°ä¼˜äºæ™®é€š RNN

---

## ğŸ§° æŠ€æœ¯æ ˆ

- **æ¡†æ¶å·¥å…·**ï¼šPyTorch, NumPy, Matplotlib, argparse
- **æ¨¡å‹ç»“æ„**ï¼šSVM, MLP, CNN, RNN, LSTM
- **å¯è§†åŒ–æ–¹æ³•**ï¼šéšè—çŠ¶æ€è½¨è¿¹å›¾ã€æ¿€æ´»çƒ­å›¾ã€è¾“å‡ºåˆ†å¸ƒå›¾ç­‰

---

## ğŸ“ Project Structure

```bash
â”œâ”€â”€ NetLin / NetFull / NetConv # CNN/MLP models for KMNIST classification
â”œâ”€â”€ check.py / check_main.py # Manually weighted MLP + hidden unit visualization
â”œâ”€â”€ anbn.py # Sequence generator for structured grammar
â”œâ”€â”€ RNN + LSTM + visual outputs # Sequence modeling and interpretability tools
â”œâ”€â”€ plot/ # Visual outputs (activation maps, trajectories, etc.)
â”œâ”€â”€ hw1.pdf # Final report and analysis
```

---

## ğŸ§  Features and Results Overview

### 1. Image Classification (KMNIST)

- Three models: Linear (NetLin), MLP (NetFull), CNN (NetConv)
- Accuracy: **70% / 85% / 94%**
- Fully implemented with PyTorch using custom training loops

### 2. Visualizing MLP Decision Logic

- Designed a 2-layer MLP with manually configured weights
- Visualized activation regions of each hidden neuron in input space
- Achieved **100% binary classification accuracy**

### 3. Sequence Modeling and Grammar Learning

- Built sequence models to learn anbn and anbncn patterns with RNN
- Visualized hidden state dynamics and softmax prediction outputs
- Applied LSTM to learn Reber Grammar with accurate state transitions, outperforming plain RNNs

---

## ğŸ§° Tech Stack

- **Frameworks**: PyTorch, NumPy, Matplotlib, argparse
- **Models**: SVM, MLP, CNN, RNN, LSTM
- **Visualization**: Hidden state trajectories, activation heatmaps, output distributions

